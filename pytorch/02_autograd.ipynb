{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bec1b87",
   "metadata": {},
   "source": [
    "**torch.autograd** Autograd là công cụ tự động tính đạo hàm của PyTorch, hỗ trợ huấn luyện mạng nơ-ron. Trong phần này, bạn sẽ hiểu được khái niệm về cách Autograd giúp huấn luyện mạng nơ-ron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a0870",
   "metadata": {},
   "source": [
    "Hãy cùng xem xét một bước huấn luyện đơn lẻ. Trong ví dụ này, chúng ta tải một mô hình resnet18 được huấn luyện trước từ torchvision. Chúng ta tạo một tensor dữ liệu ngẫu nhiên để biểu diễn một hình ảnh duy nhất với 3 kênh, chiều cao và chiều rộng là 64, và được label khởi tạo tương ứng với một số giá trị ngẫu nhiên. Nhãn trong các mô hình được huấn luyện trước có hình dạng (1,1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd47c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 217MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea54d81",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta chạy dữ liệu đầu vào qua mô hình ở từng lớp của nó để đưa ra dự đoán. Đây là bước truyền tiến (forward pass )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81296af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b29e9",
   "metadata": {},
   "source": [
    "Chúng ta sử dụng dự đoán của mô hình và nhãn tương ứng để tính toán lỗi (loss). Bước tiếp theo là lan truyền ngược lỗi này qua mạng. Quá trình lan truyền ngược được bắt đầu khi chúng ta gọi .backward() tensor lỗi. Autograd sau đó tính toán và lưu trữ độ dốc cho mỗi tham số mô hình trong  huộc tính của tham số .grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adef61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c830cdf",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta tải một trình tối ưu hóa, trong trường hợp này là SGD với tốc độ học là 0.01 và động lượng là 0.9. \n",
    "Có thể hiểu động lượng ở đây là quán tính của đạo hàm trước đó tức là cộng với 0.9 lần đạo hàm trước đó vào đạo hàm hiện tại, giúp cho gradient đi nhanh hơn nếu có cùng hướng và làm giảm độ dao động giữa cacs bước gradient khác hướng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc0cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5b7b8",
   "metadata": {},
   "source": [
    "Cuối cùng, chúng ta gọi hàm .step() để bắt đầu thuật toán tối ưu hóa gradient descent. Thuật toán tối ưu hóa sẽ điều chỉnh từng tham số dựa trên độ dốc của nó được lưu trữ trong biến .grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69ba5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66043c",
   "metadata": {},
   "source": [
    "Trong mạng nơ-ron (NN), các tham số không tính toán đạo hàm thường được gọi là các tham số đóng băng . Việc \"đóng băng\" một phần mô hình rất hữu ích nếu bạn biết trước rằng mình sẽ không cần đến đạo hàm của các tham số đó (điều này mang lại một số lợi ích về hiệu suất bằng cách giảm số lần tính toán tự động đạo hàm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1660a2",
   "metadata": {},
   "source": [
    "Trong quá trình tinh chỉnh, chúng ta đóng băng hầu hết mô hình và thường chỉ sửa đổi các lớp phân loại để đưa ra dự đoán trên các nhãn mới. Hãy cùng xem qua một ví dụ nhỏ để minh họa điều này. Như trước đây, chúng ta tải một mô hình resnet18 đã được huấn luyện trước và đóng băng tất cả các tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb01a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6345f20",
   "metadata": {},
   "source": [
    "Giả sử chúng ta muốn tinh chỉnh mô hình trên một tập dữ liệu mới với 10 nhãn. Trong ResNet, lớp phân loại là lớp tuyến tính cuối cùng model.fc. Chúng ta có thể đơn giản thay thế nó bằng một lớp tuyến tính mới (mặc định không bị đóng băng) hoạt động như lớp phân loại của chúng ta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ce17e",
   "metadata": {},
   "source": [
    "Giờ đây, tất cả các tham số trong mô hình, ngoại trừ các tham số của model.fc, đều được cố định. Các tham số duy nhất tính toán độ dốc là trọng số và độ lệch của model.fc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
